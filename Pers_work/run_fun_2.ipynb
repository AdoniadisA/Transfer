{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def raster(event_name,cellid,event_times,rasterfs,PreEndTime=200,PostBegTime=500,split=False):\n",
    "    # spike_dict : spikes of a hole recording\n",
    "    # unitidx : unit's spikes we want to look at\n",
    "    # stim_dict : stimuli, used for the names of the events\n",
    "    # eventidx : idx of event we want to look at\n",
    "    # rasterfs : spike frequency\n",
    "    \n",
    "    binlen=1.0/rasterfs\n",
    "    h=np.array([])\n",
    "    ff = (event_times['name']==event_name)\n",
    "    ## pull out each epoch from the spike times, generate a raster of spike rate\n",
    "    halfNb = int(event_times.loc[ff].shape[0]/2)        \n",
    "    \n",
    "    m = np.empty([2,PostBegTime-PreEndTime])\n",
    "    \n",
    "    for idx,(i,d) in enumerate(event_times.loc[ff].iterrows()):\n",
    "        edges=np.arange(d['start']+PreEndTime/rasterfs,d['start']+PostBegTime/rasterfs+binlen,binlen)\n",
    "        th,e=np.histogram(spike_dict[cellid],edges)\n",
    "        th=np.reshape(th,[1,-1])\n",
    "        if h.size==0:\n",
    "            # lazy hack: intialize the raster matrix without knowing how many bins it will require\n",
    "            h=th\n",
    "        else:\n",
    "            # concatenate this repetition, making sure binned length matches\n",
    "            if th.shape[1]<h.shape[1]:\n",
    "                h=np.concatenate((h,np.zeros([1,h.shape[1]])),axis=0)\n",
    "                h[-1,:]=np.nan\n",
    "                h[-1,:th.shape[1]]=th\n",
    "            else:\n",
    "                h=np.concatenate((h,th[:,:h.shape[1]]),axis=0)\n",
    "        if idx == halfNb-1 and split==True:\n",
    "            m[0,:] = np.nanmean(h,axis=0)[0:m.shape[1]]\n",
    "            h=np.array([])\n",
    "\n",
    "    if split==True:\n",
    "        m[1,:] = np.nanmean(h,axis=0)[0:m.shape[1]]\n",
    "    else:\n",
    "        m = np.nanmean(h,axis=0)[0:m.shape[1]]\n",
    "\n",
    "    return h,m\n",
    "\n",
    "def getTrainTestTimes(event_times,trainNb,testNb):\n",
    "    # event_times : timings of events \n",
    "    # trainNb : number of stimuli presented for the trains\n",
    "    # testNb : number of stimuli presented for the tests\n",
    "    \n",
    "    wavEvents = event_times[event_times['name'].str.contains('.wav')]\n",
    "    occurences =  wavEvents['name'].value_counts(sort=True)\n",
    "\n",
    "    Train_names = list(occurences[occurences==trainNb].index)\n",
    "    Test_names = list(occurences[occurences==testNb].index)\n",
    "    if Train_names == [] or Test_names == [] :\n",
    "        raise ValueError('wrong trainNb or testNb')        \n",
    "    \n",
    "    Train_times = pd.DataFrame(columns={'name','start','end'})\n",
    "    Train_times = Train_times[['name','start','end']] #Order the columns\n",
    "    Test_times = Train_times.copy()\n",
    "\n",
    "    #Get stimuli onset and offset times for trains\n",
    "    trial_indexs = event_times['name'][event_times['name']=='TRIAL'].index\n",
    "    idx1 = 0; idx2 = 0;\n",
    "    for trial_idx in trial_indexs:\n",
    "        name = event_times.iloc[trial_idx+1]['name']\n",
    "        if name in Train_names :\n",
    "            Train_times.at[idx1,'name'] = name\n",
    "            Train_times.at[idx1,'start'] = event_times.iloc[trial_idx+3]['end']\n",
    "            Train_times.at[idx1,'end'] = event_times.iloc[trial_idx+4]['start']\n",
    "            idx1 +=1\n",
    "        elif name in Test_names :\n",
    "            Test_times.at[idx2,'name'] = name\n",
    "            Test_times.at[idx2,'start'] = event_times.iloc[trial_idx+3]['end']\n",
    "            Test_times.at[idx2,'end'] = event_times.iloc[trial_idx+4]['start']\n",
    "            idx2 +=1\n",
    "        else : \n",
    "            raise ValueError('Neither a Test nor a Train stimuli name')\n",
    "\n",
    "    #Train_times = Train_times.sort_values('name')\n",
    "    #Test_times = Test_times.sort_values('name')\n",
    "\n",
    "    return Train_times,Test_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInsOuts(cellidx,event_times,fs_spectro,stim_dict,boolFigure=False):\n",
    "    print('Fetching ins & outs...\\n')\n",
    "    # fix random seed for reproducibility\n",
    "    np.random.seed(7)\n",
    "\n",
    "    # Segregate Train and Tests\n",
    "    Train_times,Test_times = getTrainTestTimes(event_times,3,24)\n",
    "\n",
    "    # Compute on a specific cell --> TODO : for all\n",
    "    cellid = list(spike_dict.keys())[cellidx] \n",
    "\n",
    "    #_________________Training input (X)_______________________#\n",
    "    stim_shape = np.shape(stim_dict[list(stim_dict.keys())[0]])\n",
    "    nbTrains = len(set(Train_times['name']))\n",
    "\n",
    "    # sound_time : from end of prestimsilence and beg of poststimsilence\n",
    "    PreStimidx = list(event_times['name']).index('PreStimSilence')\n",
    "    Endidx = event_times.columns.get_loc('end')\n",
    "    PostBegTime = int(event_times.iloc[PreStimidx+1,Endidx-1]*fs_spectro)\n",
    "    PreEndTime = int(event_times.iloc[PreStimidx,Endidx]*fs_spectro)\n",
    "    sound_time = PostBegTime - PreEndTime\n",
    "\n",
    "    X = np.zeros( (nbTrains,sound_time,stim_shape[0]) )\n",
    "    for idx,event_name in enumerate(set(Train_times['name'])):\n",
    "        X[idx,:,:] = np.transpose(stim_dict[event_name][:,PreEndTime:PostBegTime]) \n",
    "    X = X/X.max()\n",
    "\n",
    "    #_________________Training output (Y)_______________________#\n",
    "    Y = np.zeros( (nbTrains,sound_time,1) )\n",
    "    for idx,event_name in enumerate(set(Train_times['name'])):\n",
    "        h,m = raster(event_name,cellid,event_times,options['rasterfs'],PreEndTime,PostBegTime)\n",
    "        Y[idx,:,0] = np.transpose(m[0:np.size(Y,1)])\n",
    "    Y = Y/Y.max()\n",
    "\n",
    "    #_________________TEST input (W)_______________________#\n",
    "    nbTests = len(set(Test_times['name']))\n",
    "    W = np.zeros( (nbTests,sound_time,stim_shape[0]) )\n",
    "    for idx,event_name in enumerate(set(Test_times['name'])):\n",
    "        W[idx,:,:] = np.transpose(stim_dict[event_name][:,PreEndTime:PostBegTime]) \n",
    "    W = W/W.max()\n",
    "\n",
    "    #_________________Test output (Z)_______________________#\n",
    "    Z = np.zeros( (nbTests,sound_time,2) )\n",
    "    t_12 = np.zeros(3)\n",
    "    for idx,event_name in enumerate(set(Test_times['name'])):\n",
    "        h,m = raster(event_name,cellid,event_times,options['rasterfs'],PreEndTime,PostBegTime,split=True)\n",
    "        Z[idx,:,:] = np.swapaxes(m,0,1)\n",
    "        if boolFigure :\n",
    "            plt.figure()\n",
    "            plt.plot(Z[idx,:,0])\n",
    "            plt.plot(Z[idx,:,1])\n",
    "            plt.title('Split spike rates from sound{}'.format(idx))\n",
    "        t_12[idx] = np.corrcoef(Z[idx,:,0],Z[idx,:,1])[0,1]\n",
    "        print('Correlation between sound{}\\'s split spike rates: {}'.format(idx,t_12[idx]))\n",
    "\n",
    "    return X,Y,W,Z,t_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_and_fit_conv1Dmodel(X,Y,epochs,batch_size,time_window,kernel_init,lr,activation,loss,optim,validation_split,early_stop=False):\n",
    "    #### KERAS 1D CONV MODEL\n",
    "    #import os\n",
    "    #os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "    #os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "    #import tensorflow as tf\n",
    "    #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.01)\n",
    "    #sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "    # Create your first MLP in Keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Conv1D,Dense\n",
    "    from keras.layers.advanced_activations import LeakyReLU\n",
    "    from keras.constraints import non_neg\n",
    "    import keras.initializers\n",
    "    import keras.optimizers\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    layer = Conv1D(input_shape=np.shape(X)[1:3],filters=1,kernel_size=time_window,strides=1,\n",
    "                padding='causal',activation='relu',dilation_rate=1,use_bias=True,\n",
    "                bias_initializer='random_uniform')\n",
    "\n",
    "    layer.kernel_initializer = keras.initializers.RandomUniform(minval=kernel_init[0], maxval=kernel_init[1], seed=None)\n",
    "    model.add(layer)\n",
    "\n",
    "    # Compile model\n",
    "    sgd = getattr(keras.optimizers,optim)(lr=lr)\n",
    "    model.compile(loss = loss, optimizer=sgd)\n",
    "\n",
    "    # Fit the model\n",
    "    early_cbk = []\n",
    "    if early_stop:\n",
    "        early_cbk = [keras.callbacks.EarlyStopping(patience = 20,verbose=1)]\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X,Y,validation_split = validation_split,epochs=epochs, batch_size=batch_size, verbose=0)#, callbacks = early_cbk)\n",
    "    print('Elapsed fitting time : {}'.format(time.time() - start_time))\n",
    "\n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare out and predicted\n",
    "def prediction_score_and_plots(W,predicted,Y,Z,onTest=True,fig = True):\n",
    "    import random as rand\n",
    "    if fig:\n",
    "        plt.plot(history.history['loss'])\n",
    "        if 'val_loss' in history.history:\n",
    "            plt.plot(history.history['val_loss'])\n",
    "        #plt.plot(history_early.history['val_loss'],'--')\n",
    "        plt.legend(('loss','loss with validation','validation_loss'))\n",
    "        plt.title('loss over training')\n",
    "        plt.figure\n",
    "\n",
    "    score = []\n",
    "    if onTest:\n",
    "        for idx in range(3):\n",
    "            example = idx\n",
    "            if fig :\n",
    "                plt.figure()\n",
    "                plt.plot(Z[example,:,0])\n",
    "                plt.plot(Z[example,:,1])\n",
    "                plt.plot(predicted[example,:,0])\n",
    "                plt.title(\"individual PSTH of {}th cell from sound {}\".format(cellidx,example))\n",
    "                plt.legend(('output1','output2','prediction','prediction_early_stop'))        \n",
    "            c1 = np.corrcoef(Z[example,:,0],predicted[example,:,0])[0,1]\n",
    "            c2 = np.corrcoef(Z[example,:,1],predicted[example,:,0])[0,1]\n",
    "            score.append( (c1**2/2 + c2**2/2)/t_12[example] )\n",
    "            print('Explained Score for sound {}: {}'.format(example,score[example]))\n",
    "    else :\n",
    "        for idx in range(5):\n",
    "            example = rand.randint(0,Y.shape[0]-1)   \n",
    "            if fig:\n",
    "                plt.figure()\n",
    "                plt.plot(Y[example,:,0])\n",
    "                plt.plot(predicted[example,:,0])\n",
    "                plt.title(\"individual PSTH of {}th cell from sound {}\".format(cellidx,example))\n",
    "                plt.legend(('output','prediction'))\n",
    "\n",
    "    #Plot STRF\n",
    "    dirac_spec = np.concatenate((np.ones((X.shape[2],1)),np.zeros((X.shape[2],time_window-1))),axis=1)\n",
    "    weights = model.get_weights()[0].squeeze().transpose()\n",
    "\n",
    "    STRF = np.zeros(weights.shape)\n",
    "    for idx in range(weights.shape[0]):\n",
    "        conv = np.convolve(dirac_spec[idx],weights[idx],mode='full')\n",
    "        STRF[idx][:] = conv[0:weights.shape[1]]\n",
    "\n",
    "    if fig:\n",
    "        plt.figure()\n",
    "        plt.imshow(STRF)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached stim: /auto/data/tmp/tstim/NaturalSounds-2-0.5-3-1-White______-100-0-3__8-65dB-ozgf-fs100-ch18-incps1.mat\n",
      "Spike file: /auto/data/daq/Tartufo/TAR010/sorted/TAR010c16_p_NAT.spk.mat\n",
      "rounding Trial offset spike times to even number of rasterfs bins\n",
      "342 trials totaling 2076.12 sec\n",
      "Creating trial events\n",
      "Creating trial outcome events\n",
      "Removing post-response stimuli\n",
      "Keeping 2394/2394 events that precede responses\n"
     ]
    }
   ],
   "source": [
    "import os, io, re, scipy.io, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nems.utilities as nu\n",
    "import nems.db as nd\n",
    "import nems.utilities.baphy\n",
    "\n",
    "parmfilepath='/auto/data/daq/Tartufo/TAR010/TAR010c16_p_NAT.m'\n",
    "\n",
    "options={'rasterfs': 100, 'includeprestim': True, 'stimfmt': 'ozgf', 'chancount': 18, 'cellid': 'all', 'pupil': True}\n",
    "event_times, spike_dict, stim_dict, state_dict = nu.baphy.baphy_load_recording(parmfilepath,options)\n",
    "\n",
    "X,Y,W,Z,t_12 = getInsOuts(21,event_times,options['rasterfs'],stim_dict,False)\n",
    "\n",
    "model,history = mk_and_fit_conv1Dmodel(X,Y,1000,40,10,[0,0.001],0.005,'relu','poisson','SGD',0.2,False)\n",
    "predicted = model.predict(W)\n",
    "\n",
    "score = prediction_score_and_plots(W,predicted,Y,Z,onTest=True,fig = True)\n",
    "\n",
    "save = False\n",
    "if save:\n",
    "    # SAVE MODEL\n",
    "    config = model.get_config()[0]\n",
    "    print(config)\n",
    "    save_name = config['class_name'] + '_'  + config['config']['activation'] + '_compiler-' + \\\n",
    "        loss + '-' + str(lr) + '_'+ '_ker-' + \\\n",
    "        str(config['config']['kernel_initializer']['config']['minval']) + '-' + \\\n",
    "        str(config['config']['kernel_initializer']['config']['maxval']) + '_' + \\\n",
    "        'epochs-' + str(epochs) + '_batch-' + str(batch_size) \n",
    "\n",
    "    model.save('STRF_computation/models_trained/' + save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "#import tensorflow as tf\n",
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
